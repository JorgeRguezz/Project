{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Free up CUDA memory at the start\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check GPU availability and set memory optimization flags\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB\")\n",
    "    print(f\"Memory reserved: {torch.cuda.memory_reserved(0) / 1e6:.2f} MB\")\n",
    "\n",
    "# T4-specific optimizations for PyTorch\n",
    "torch.backends.cudnn.benchmark = True  # Speed up training if input sizes don't change\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # More deterministic behavior\n",
    "\n",
    "# Disable Weights & Biases logging\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Function to free up GPU memory\n",
    "def free_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Memory allocated after cleanup: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB\")\n",
    "\n",
    "# Load the dataset with caching to avoid reloading\n",
    "print(\"Loading MMLU-Pro dataset...\")\n",
    "cache_dir = \"/content/cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\", cache_dir=cache_dir)\n",
    "\n",
    "# Extract questions and categories\n",
    "print(\"Extracting questions and categories...\")\n",
    "preguntas = [item[\"question\"] for item in dataset[\"test\"]]\n",
    "categorias = [item[\"category\"] for item in dataset[\"test\"]]\n",
    "\n",
    "# Check category distribution\n",
    "category_counts = Counter(categorias)\n",
    "print(\"Category distribution:\")\n",
    "for category, count in category_counts.most_common():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "# Get unique categories\n",
    "categorias_unicas = list(set(categorias))\n",
    "categorias_a_id = {categoria: i for i, categoria in enumerate(categorias_unicas)}\n",
    "\n",
    "# Save category mapping\n",
    "with open('categorias_a_id.json', 'w') as f:\n",
    "    json.dump(categorias_a_id, f)\n",
    "\n",
    "# Format prompts with instructional prefix to improve model understanding\n",
    "def format_prompt(question):\n",
    "    return f\"Classify the academic category: {question}\"\n",
    "\n",
    "# Apply prompt formatting\n",
    "formatted_questions = [format_prompt(q) for q in preguntas]\n",
    "category_ids = [categorias_a_id[cat] for cat in categorias]\n",
    "\n",
    "# Choose the most efficient model for T4 GPU\n",
    "# DistilBERT is smaller and faster than BERT, while maintaining good performance\n",
    "model_name = \"distilbert-base-uncased\"  # A lightweight alternative\n",
    "# Alternatively, you could use SciBERT if you need domain-specific understanding\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Dataset class for the model with memory optimization\n",
    "class MMLUProDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Use a single fold for faster results, or multiple folds for better evaluation\n",
    "use_cross_validation = False  # Set to True if you want cross-validation\n",
    "\n",
    "if use_cross_validation:\n",
    "    n_splits = 3  # Reduced from 5 to save time on T4\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    \n",
    "    print(f\"Starting {n_splits}-fold cross-validation...\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(formatted_questions)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Free memory before starting a new fold\n",
    "        free_gpu_memory()\n",
    "        \n",
    "        # Split data for this fold\n",
    "        fold_train_texts = [formatted_questions[i] for i in train_idx]\n",
    "        fold_train_labels = [category_ids[i] for i in train_idx]\n",
    "        fold_val_texts = [formatted_questions[i] for i in val_idx]\n",
    "        fold_val_labels = [category_ids[i] for i in val_idx]\n",
    "        \n",
    "        # Skip data augmentation for speed on T4\n",
    "        # Tokenize data - use a smaller max_length to conserve memory\n",
    "        print(\"Tokenizing data...\")\n",
    "        max_length = 256  # Reduced from 512 to save memory on T4\n",
    "        train_encodings = tokenizer(fold_train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "        val_encodings = tokenizer(fold_val_texts, truncation=True, padding=True, max_length=max_length)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = MMLUProDataset(train_encodings, fold_train_labels)\n",
    "        val_dataset = MMLUProDataset(val_encodings, fold_val_labels)\n",
    "        \n",
    "        # Compute class weights to handle imbalance\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(fold_train_labels), y=fold_train_labels)\n",
    "        class_weights_dict = {i: float(weight) for i, weight in enumerate(class_weights)}\n",
    "        \n",
    "        # Create model with memory optimization\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=len(categorias_unicas)\n",
    "        )\n",
    "        \n",
    "        # T4-optimized training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results/fold_{fold+1}',\n",
    "            evaluation_strategy=\"steps\",  # Changed from epoch to steps for more frequent evaluation\n",
    "            eval_steps=100,  # Evaluate every 100 steps\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=100,\n",
    "            save_total_limit=2,  # Keep only the 2 best checkpoints to save space\n",
    "            learning_rate=5e-5,  # Slightly higher learning rate\n",
    "            per_device_train_batch_size=16,  # Adjusted for T4 memory\n",
    "            per_device_eval_batch_size=32,\n",
    "            gradient_accumulation_steps=2,  # Effective batch size = 16*2 = 32\n",
    "            num_train_epochs=3,  # Reduced epochs for T4\n",
    "            warmup_ratio=0.1,  # Use ratio instead of steps\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f'./logs/fold_{fold+1}',\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_f1\",\n",
    "            greater_is_better=True,\n",
    "            fp16=True,  # Mixed precision training for T4\n",
    "            dataloader_num_workers=2,  # Parallel data loading\n",
    "            dataloader_pin_memory=True,  # Faster data transfer to GPU\n",
    "            report_to=\"none\"  # Disable wandb explicitly\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training model for fold {fold+1}...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(f\"Evaluating model for fold {fold+1}...\")\n",
    "        eval_result = trainer.evaluate()\n",
    "        fold_results.append(eval_result)\n",
    "        \n",
    "        print(f\"Fold {fold+1} results: {eval_result}\")\n",
    "        \n",
    "        # Save the model for this fold\n",
    "        model.save_pretrained(f\"modelo_mmlu_fold_{fold+1}\")\n",
    "        tokenizer.save_pretrained(f\"modelo_mmlu_fold_{fold+1}\")\n",
    "        \n",
    "        # Free up memory after each fold\n",
    "        del model, trainer, train_dataset, val_dataset\n",
    "        free_gpu_memory()\n",
    "    \n",
    "    # Print cross-validation results\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    avg_accuracy = sum(result['eval_accuracy'] for result in fold_results) / len(fold_results)\n",
    "    avg_f1 = sum(result['eval_f1'] for result in fold_results) / len(fold_results)\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "# Train final model on entire dataset\n",
    "print(\"\\nTraining final model...\")\n",
    "free_gpu_memory()\n",
    "\n",
    "# Format all questions\n",
    "all_formatted_questions = [format_prompt(q) for q in preguntas]\n",
    "all_labels = [categorias_a_id[cat] for cat in categorias]\n",
    "\n",
    "# Use 90% for training, 10% for final validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    all_formatted_questions, all_labels, test_size=0.1, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "# Simple data augmentation (limited for T4 efficiency)\n",
    "print(\"Applying targeted data augmentation...\")\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Only augment underrepresented classes\n",
    "label_counts = Counter(train_labels)\n",
    "median_count = np.median(list(label_counts.values()))\n",
    "underrepresented_labels = [label for label, count in label_counts.items() if count < median_count]\n",
    "\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "for text, label in zip(train_texts, train_labels):\n",
    "    if label in underrepresented_labels:\n",
    "        try:\n",
    "            augmented_text = aug.augment(text)[0]\n",
    "            augmented_texts.append(augmented_text)\n",
    "            augmented_labels.append(label)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"Added {len(augmented_texts)} augmented examples for underrepresented classes\")\n",
    "\n",
    "# Combine original and augmented data\n",
    "train_texts = train_texts + augmented_texts\n",
    "train_labels = train_labels + augmented_labels\n",
    "\n",
    "# Tokenize with optimized parameters for T4\n",
    "print(\"Tokenizing data for final model...\")\n",
    "max_length = 256  # Reduced from 512 to save memory\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MMLUProDataset(train_encodings, train_labels)\n",
    "val_dataset = MMLUProDataset(val_encodings, val_labels)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_dict = {i: float(weight) for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Create final model\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(categorias_unicas)\n",
    ")\n",
    "\n",
    "# T4-optimized training arguments for final model\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir='./results/final_model',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,  # Keep only the 2 best checkpoints\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine scheduler works well for final training\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs/final_model',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=\"none\",\n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# Create trainer for final model\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "print(\"Training final model...\")\n",
    "final_trainer.train()\n",
    "\n",
    "# Evaluate final model\n",
    "print(\"Evaluating final model...\")\n",
    "final_eval_result = final_trainer.evaluate()\n",
    "print(f\"Final model evaluation: {final_eval_result}\")\n",
    "\n",
    "# Save the final model\n",
    "print(\"Saving final model...\")\n",
    "final_model.save_pretrained(\"modelo_mmlu_final\")\n",
    "tokenizer.save_pretrained(\"modelo_mmlu_final\")\n",
    "\n",
    "# Create a pipeline for easy inference\n",
    "from transformers import pipeline\n",
    "print(\"Creating classification pipeline...\")\n",
    "clasificador = pipeline(\"text-classification\", model=\"modelo_mmlu_final\", tokenizer=\"modelo_mmlu_final\")\n",
    "\n",
    "# Get reverse mapping from ID to category name\n",
    "id_to_category = {id_val: cat for cat, id_val in categorias_a_id.items()}\n",
    "\n",
    "# Test on a few examples\n",
    "print(\"\\nTesting the model with examples:\")\n",
    "test_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who discovered America?\",\n",
    "    \"What is an index fund?\",\n",
    "    \"What is 2+2?\",\n",
    "    \"What is the chemical symbol for water?\",\n",
    "    \"Who wrote Hamlet?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    formatted_question = format_prompt(question)\n",
    "    result = clasificador(formatted_question)\n",
    "    label_id = int(result[0]['label'].split('_')[1])\n",
    "    category_name = id_to_category.get(label_id, \"Unknown\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted category: {category_name} (confidence: {result[0]['score']:.4f})\\n\")\n",
    "\n",
    "# Function to evaluate model on a larger test set\n",
    "def evaluate_model_performance(model_path, test_data, test_labels, category_mapping):\n",
    "    \"\"\"Evaluate model performance on a test set\"\"\"\n",
    "    # Create a classification pipeline\n",
    "    test_classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = []\n",
    "    for question in test_data:\n",
    "        formatted_question = format_prompt(question)\n",
    "        result = test_classifier(formatted_question, truncation=True, max_length=512)\n",
    "        label_id = int(result[0]['label'].split('_')[1])\n",
    "        predictions.append(label_id)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    \n",
    "    # Get classification report\n",
    "    id_to_category = {v: k for k, v in category_mapping.items()}\n",
    "    target_names = [id_to_category[i] for i in range(len(id_to_category))]\n",
    "    \n",
    "    report = classification_report(test_labels, predictions, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "# Evaluate final model on a separate test set\n",
    "print(\"\\nEvaluating final model on test set...\")\n",
    "_, test_texts, _, test_labels = train_test_split(\n",
    "    all_formatted_questions, all_labels, test_size=0.2, random_state=24, stratify=all_labels\n",
    ")\n",
    "\n",
    "accuracy, report = evaluate_model_performance(\"modelo_mmlu_final\", test_texts, test_labels, categorias_a_id)\n",
    "print(f\"Test set accuracy: {accuracy:.4f}\")\n",
    "print(\"Performance by category:\")\n",
    "for category, metrics in report.items():\n",
    "    if category not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        print(f\"{category}: F1-score = {metrics['f1-score']:.4f}, Precision = {metrics['precision']:.4f}, Recall = {metrics['recall']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "torch.save(model.state_dict(), 'modelo_entrenado.pth')\n",
    "\n",
    "# Código para descargar el archivo\n",
    "from google.colab import files\n",
    "files.download('modelo_entrenado.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
