{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEnrJwqIIoVm",
        "outputId": "fd90ae9c-edf5-45e1-a4ab-715ce55e09d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9vU_lz8-1Ji"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/cache/*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxpYb66vifSH"
      },
      "source": [
        "# **Cargar y preparar los datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9tacHz4IYRF",
        "outputId": "f827607c-31e9-4b9b-9994-83da26f457a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'question_id': 70, 'question': 'Typical advertising regulatory bodies suggest, for example that adverts must not: encourage _________, cause unnecessary ________ or _____, and must not cause _______ offence.', 'options': ['Safe practices, Fear, Jealousy, Trivial', 'Unsafe practices, Distress, Joy, Trivial', 'Safe practices, Wants, Jealousy, Trivial', 'Safe practices, Distress, Fear, Trivial', 'Unsafe practices, Wants, Jealousy, Serious', 'Safe practices, Distress, Jealousy, Serious', 'Safe practices, Wants, Fear, Serious', 'Unsafe practices, Wants, Fear, Trivial', 'Unsafe practices, Distress, Fear, Serious'], 'answer': 'I', 'answer_index': 8, 'cot_content': '', 'category': 'business', 'src': 'ori_mmlu-business_ethics'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset de MMLU-Pro\n",
        "dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
        "print(dataset[\"test\"][0])\n",
        "\n",
        "# Extract the questions and categories from the dataset\n",
        "preguntas = [item[\"question\"] for item in dataset[\"test\"]]\n",
        "categorias = [item[\"category\"] for item in dataset[\"test\"]]\n",
        "\n",
        "# Split the dataset into training and validation sets (80% para entrenar, 20% para validarlo)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(preguntas, categorias, test_size=0.2)\n",
        "# train_texts y val_texts preguntas para entrenar y validar (respectivamente)\n",
        "# train_labels y val_labels categorÃ­as para entrenar y validar (respectivamente)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFbjKIoiq6s"
      },
      "source": [
        "# **Tokenizar los datos**\n",
        "Convertir el texto en nÃºmeros que el modelo pueda entender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owFbDyYLIjb0"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQng_ah1jBfR"
      },
      "source": [
        "# **Crear el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzYYCSP4I-5s",
        "outputId": "f981b8fc-1a98-43c8-f224-6d8ecf8c04ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "num_categorias = len(set(categorias))\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_categorias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eVumf3KjL0_"
      },
      "source": [
        "**Convertir categorias a nÃºmeros**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FOPNQk-K1-l"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Obtener todas las categorÃ­as Ãºnicas\n",
        "categorias_unicas = list(set(categorias))\n",
        "\n",
        "# Crear un diccionario {categoria: id}\n",
        "categorias_a_id = {categoria: i for i, categoria in enumerate(categorias_unicas)}\n",
        "\n",
        "# Guardar el mapeo\n",
        "with open('categorias_a_id.json', 'w') as f:\n",
        "    json.dump(categorias_a_id, f)\n",
        "\n",
        "# Convertir las categorÃ­as a ids\n",
        "train_labels = [categorias_a_id[label] for label in train_labels]\n",
        "val_labels = [categorias_a_id[label] for label in val_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebSozHw-jV2o"
      },
      "source": [
        "Desabilitar Wandb (Weights & Biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX81QF6RMdYV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjrmVCKyjo1w"
      },
      "source": [
        "# **Entrenar el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "XbihZSRBK_lM",
        "outputId": "89cfda2d-2412-43ea-90cf-42f6f274501f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3612' max='3612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3612/3612 49:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.733200</td>\n",
              "      <td>0.629607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.346200</td>\n",
              "      <td>0.630594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.131600</td>\n",
              "      <td>0.686813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3612, training_loss=0.46433605096928965, metrics={'train_runtime': 2944.6812, 'train_samples_per_second': 9.806, 'train_steps_per_second': 1.227, 'total_flos': 7598150284032000.0, 'train_loss': 0.46433605096928965, 'epoch': 3.0})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Convert the labels to tensor\n",
        "class MMLUProDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # labels need to be of type long\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = MMLUProDataset(train_encodings, train_labels)\n",
        "val_dataset = MMLUProDataset(val_encodings, val_labels)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',         # output directory\n",
        "    evaluation_strategy=\"epoch\",    # evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",          # save at the end of each epoch\n",
        "    per_device_train_batch_size=8,  # batch size for training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    num_train_epochs=5, \n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    logging_dir='./logs',\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    compute_metrics=compute_metrics, \n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset, \n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]# evaluation dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRsu50Toj8_6"
      },
      "source": [
        "# **Guardar y usar el modelo entrenado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN6Cn2Rxd_3T",
        "outputId": "02ba9e65-5be0-4909-af9e-da8dea5fa2e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"modelo_mmlu\")\n",
        "tokenizer.save_pretrained(\"modelo_mmlu\")\n",
        "\n",
        "from transformers import pipeline\n",
        "clasificador = pipeline(\"text-classification\", model= \"modelo_mmlu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyHcT-1zkDHr"
      },
      "source": [
        "# **Pruebas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6eIRU_IhPQ0",
        "outputId": "954d6322-7030-4594-a3b9-66a9365ea94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'LABEL_9', 'score': 0.9982511401176453}]\n",
            "[{'label': 'LABEL_10', 'score': 0.7519559860229492}]\n",
            "[{'label': 'LABEL_11', 'score': 0.7832615971565247}]\n",
            "[{'label': 'LABEL_4', 'score': 0.9984478950500488}]\n",
            "[{'label': 'LABEL_13', 'score': 0.9808743596076965}]\n",
            "[{'label': 'LABEL_9', 'score': 0.9969940185546875}]\n"
          ]
        }
      ],
      "source": [
        "print(clasificador(\"What is the capital of France?\"))\n",
        "print(clasificador(\"Who discovered America?\"))\n",
        "print(clasificador(\"What is an index fund?\"))\n",
        "print(clasificador(\"What is 2+2?\"))\n",
        "print(clasificador(\"What is the chemical symbol for water?\"))\n",
        "print(clasificador(\"Who wrote Hamlet?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aruaLp0fU6u",
        "outputId": "1511a877-b27c-467f-db0c-3c5717aef432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('psychology', 0), ('philosophy', 1), ('computer science', 2), ('biology', 3), ('math', 4), ('health', 5), ('business', 6), ('physics', 7), ('engineering', 8), ('other', 9), ('history', 10), ('economics', 11), ('law', 12), ('chemistry', 13)]\n"
          ]
        }
      ],
      "source": [
        "print(list(categorias_a_id.items()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niCsmZfwhRcF"
      },
      "source": [
        "# **Evaluar tasa de aciertos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t-QpnznhWMH",
        "outputId": "e2ad0a3e-dac2-4b45-cb8b-6e37d869993f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preguntas tras el filtrado: 999\n",
            "Tasa de aciertos: 0.00%\n"
          ]
        }
      ],
      "source": [
        "# from transformers import pipeline\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from datasets import load_dataset\n",
        "# from random import sample\n",
        "# import json\n",
        "\n",
        "# #cargar mapeo de categorias\n",
        "# with open('categorias_a_id.json', 'r') as f:\n",
        "#     categorias_a_id = json.load(f)\n",
        "\n",
        "# # Cargar el dataset\n",
        "# dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
        "# val_dataset = dataset[\"test\"]\n",
        "# random_val_dataset = sample(list(val_dataset), 1000)\n",
        "\n",
        "# clasificador = pipeline(\"text-classification\", model=\"modelo_mmlu\", tokenizer=\"modelo_mmlu\", device=0)\n",
        "\n",
        "# def filtrar_preguntas(dataset, max_tokens=512):\n",
        "#     \"\"\" Filtra preguntas que tengan menos de max_tokens al tokenizarlas. \"\"\"\n",
        "#     preguntas_filtradas = []\n",
        "#     for item in dataset:\n",
        "#         num_tokens = len(clasificador.tokenizer.encode(item[\"question\"], truncation=False))\n",
        "#         if num_tokens <= max_tokens:\n",
        "#             preguntas_filtradas.append(item)\n",
        "#     return preguntas_filtradas\n",
        "\n",
        "# # Filtrar preguntas largas\n",
        "# max_tokens = 512\n",
        "# dataset_filtrado = filtrar_preguntas(random_val_dataset, max_tokens)\n",
        "# print(f\"Preguntas tras el filtrado: {len(dataset_filtrado)}\")\n",
        "\n",
        "# # Obtener predicciones del modelo para las preguntas filtradas\n",
        "# predicciones = []\n",
        "# categorias_reales = []\n",
        "\n",
        "# for item in dataset_filtrado:\n",
        "#     pregunta = item[\"question\"]\n",
        "#     categoria_real = item[\"category\"]\n",
        "\n",
        "#     resultado = clasificador(pregunta, truncation=True, max_length=max_tokens)  # Truncar la entrada al modelo\n",
        "#     etiqueta_predicha = int(resultado[0]['label'].split('_')[1])  # Convertir LABEL_X a nÃºmero\n",
        "#     categorias_reales.append(categoria_real)\n",
        "#     predicciones.append(etiqueta_predicha)\n",
        "\n",
        "# # Calcular la precisiÃ³n\n",
        "# tasa_aciertos = accuracy_score(categorias_reales, predicciones)\n",
        "# print(f\"Tasa de aciertos: {tasa_aciertos * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29msdQ_-_8vU",
        "outputId": "3ca7ce90-5023-4d95-df90-9c2af0a54684"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preguntas tras el filtrado: 999\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import load_dataset\n",
        "from random import sample\n",
        "import json\n",
        "\n",
        "# Cargar el mapeo de categorÃ­as\n",
        "with open(\"categorias_a_id.json\", \"r\") as f:\n",
        "    categorias_a_id = json.load(f)\n",
        "\n",
        "# Cargar el dataset\n",
        "dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
        "val_dataset = dataset[\"test\"]\n",
        "random_val_dataset = sample(list(val_dataset), 1000)\n",
        "\n",
        "clasificador = pipeline(\"text-classification\", model=\"modelo_mmlu\", tokenizer=\"modelo_mmlu\", device=0)\n",
        "\n",
        "def filtrar_preguntas(dataset, max_tokens=512):\n",
        "    preguntas_filtradas = []\n",
        "    for item in dataset:\n",
        "        num_tokens = len(clasificador.tokenizer.encode(item[\"question\"], truncation=False))\n",
        "        if num_tokens <= max_tokens:\n",
        "            preguntas_filtradas.append(item)\n",
        "    return preguntas_filtradas\n",
        "\n",
        "max_tokens = 512\n",
        "dataset_filtrado = filtrar_preguntas(random_val_dataset, max_tokens)\n",
        "print(f\"Preguntas tras el filtrado: {len(dataset_filtrado)}\")\n",
        "\n",
        "predicciones = []\n",
        "categorias_reales = []\n",
        "\n",
        "for item in dataset_filtrado:\n",
        "    pregunta = item[\"question\"]\n",
        "    categoria_real = item[\"category\"]\n",
        "\n",
        "    resultado = clasificador(pregunta, truncation=True, max_length=max_tokens)\n",
        "    etiqueta_predicha = int(resultado[0]['label'].split('_')[1])  # Ej: LABEL_9 â†’ 9\n",
        "    categorias_reales.append(categoria_real)\n",
        "    predicciones.append(etiqueta_predicha)\n",
        "\n",
        "# Convertir categorÃ­as reales a IDs usando el diccionario\n",
        "categorias_reales_ids = [categorias_a_id[categoria] for categoria in categorias_reales]\n",
        "\n",
        "tasa_aciertos = accuracy_score(categorias_reales_ids, predicciones)\n",
        "print(f\"Tasa de aciertos: {tasa_aciertos * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
